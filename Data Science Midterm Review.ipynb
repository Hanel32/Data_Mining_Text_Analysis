{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how the Maximal Marginal Classifier works:\n",
    "----\n",
    "Wikipedia: In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.\n",
    "\n",
    "The notion of margin is important in several machine learning classification algorithms, as it can be used to bound the generalization error of the classifier. These bounds are frequently shown using the VC dimension. Of particular prominence is the generalization error bound on boosting algorithms and support vector machines.\n",
    "\n",
    "My own words: Contextually, it would make sense if I described this in terms of the SVM. In the SVM, the goal is to classify between more than one classes, and the method of classification is essentially finding a line, polynomial, or radial section of the graph which is homogeneous (i.e. containing all of the same classification of data, or atleast predominantly) and can be quantified. In SVM, the MMC attempts to tune the distance from the hyperplane of the data that is considered when tuning.\n",
    "\n",
    "Instead of the Support Vector Machine, here's a paper applying the MMC to the Perceptron algorithm:\n",
    "https://people.eecs.berkeley.edu/~jrs/189/lec/03.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the limitations of the Maximal Marginal Classifier?\n",
    "----\n",
    "I was confused by this question at first thinking, \"Well, it does what it does. What do you mean?\" but upon revisiting the slides, I see that this is probably asking in what situations would the MMC be a non useful method. Such cases are cases wherein there does not exist a defined linear boundary in the dataset such that two or more classes could be disambiguated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what way does the Support Vector Classifier extend MMC?\n",
    "----\n",
    "Wikipedia: In geometry, the hyperplane separation theorem is a theorem about disjoint convex sets in n-dimensional Euclidean space. There are several rather similar versions. In one version of the theorem, if both these sets are closed and at least one of them is compact, then there is a hyperplane in between them and even two parallel hyperplanes in between them separated by a gap. In another version, if both disjoint convex sets are open, then there is a hyperplane in between them, but not necessarily any gap. An axis which is orthogonal to a separating hyperplane is a separating axis, because the orthogonal projections of the convex bodies onto the axis are disjoint.\n",
    "\n",
    "The hyperplane separation theorem is due to Hermann Minkowski. The Hahn–Banach separation theorem generalizes the result to topological vector spaces.\n",
    "\n",
    "A related result is the supporting hyperplane theorem.\n",
    "\n",
    "In geometry, a maximum-margin hyperplane is a hyperplane which separates two 'clouds' of points and is at equal distance from the two. The margin between the hyperplane and the clouds is maximal. See the article on Support Vector Machines for more details.\n",
    "\n",
    "My own words: I essentially explained this relationship in question 1; the SVM creates the hyperplane based upon the maximal marginal classifier, rather, the maximal marginal classifier helps the SVM identify the values that should be considered when tuning a learning function to the dataset. For example, if you have a really well defined boundary between class \"0\" and class \"1\", but class \"1\" has a factor of 10 more values that class \"0\". Intuitively, in regression, the line of fit would be skewed in the direction of class \"1\". If, instead, you utilize a support vector classifier coupled with the maximal marginal classifier, you can tune the distance from the hyperplane which would allow the algorithm to focus on the data at the boundaries, and make the line of best fit, rather than that arbitrary linear fit. This idea extends to polynomial and radial kernels. To really put it in plain words, the MMC simply says how far from the hyperplane the SVM should look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the metrics Precision, Recall, F-score and Accuracy:\n",
    "----\n",
    "Precision: $\\frac{tp}{tp + fp}$\n",
    "\n",
    "Recall: $\\frac{tp}{tp + fn}$\n",
    "\n",
    "F-score: 2* $\\frac{precision * recall}{precision + recall}$\n",
    "\n",
    "Accuracy: $\\frac{tp + tn}{tp + tn + fp + fn}$\n",
    "\n",
    "Provide numerical examples when F1 is fairer than Accurracy:\n",
    "----\n",
    "Most obviously, F-score is best for data with stratum; Rather, not all data is cleanly 50/50 between two classes, and when it isn't, F1 is fairer than accuracy. F-score isn't affected by the true negative values, but rather simply just the true positive values, and those which were incorrectly classified.\n",
    "\n",
    "Put more aptly:\n",
    "\n",
    "https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
    "\n",
    "Main point, explaining the weakness of accuracy:\n",
    "\n",
    "\"This is called the accuracy paradox. When TP < FP, then accuracy will always increase when we change a classification rule to always output “negative” category. Conversely, when TN < FN, the same will happen when we change our rule to always output “positive”.\"\n",
    "\n",
    "Precision, Recall:\n",
    "\n",
    "\"If you think about it for a moment, precision answers the following question: out of all the examples the classifier labeled as positive, what fraction were correct? On the other hand, recall answers: out of all the positive examples there were, what fraction did the classifier pick up?\"\n",
    "\n",
    "F-score:\n",
    "\n",
    "\"The conclusion is that tweaking a classifier is a matter of balancing what is more important for us: precision or recall. It is possible to get both up: one may choose to optimize a measure that combines precision and recall into a single value, such as the F-measure, but we reach a point in which we can’t go any further and our decisions are to be influenced by other factors.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the principle of using TF-IDF for finding relevant words. Provide a numerical example.\n",
    "----\n",
    "My own words: TF-IDF is a nice little tool for finding relevant words, or moreover relevant words to works.\n",
    "\n",
    "Step one: Calculate the term frequency of each document. This can be easily done by simply tokenizing the corpus, splitting it up between \"documents\", and finding out how many times each representative token shows up in that document, and tabulate. Term frequency is important for telling us which words the author favors.\n",
    "\n",
    "Step two: Calculate the inverse document frequency. This is calculated by taking $N$, the total number of documents, and dividing it by the number of documents that contain a token, $t$ for every token in the corpus.\n",
    "\n",
    "Step three: Once you've calculated the term frequency and inverse document frequency for every token in the corpus, generate the product of the two to find the corpus' TF-IDF data.\n",
    "\n",
    "Intuition: Incredibly common words like \"I\" \"the\" \"and\" will be completely smoothed from scoring, because they show up in every document, they have a score of 0, and thus are ignored. TF-IDF gives much higher weights to those words that are unique among documents of a corpus, and effectively smoothes words that show up incredibly commonly, like colloquial human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are n-grams? Provide an example in which n-grams (n>1) is better than unigram:\n",
    "----\n",
    "My own words: N-grams are powerful in retaining the sequencing of words rather than simply looking at each individual token in an independent setting. Bigrams, and trigrams allow for the scientist to peek at the sentence structure, and nab occurrences like \"not happy\", \"not sunny\", etc. which are effective negations of the sentiment. You could utilize those negation factors to more appropriately explain the content and sentiment of the text.\n",
    "\n",
    "Example:\n",
    "\"I have not had an awesome day, it's been crazy!\"\n",
    "\n",
    "- Unigrams, the sentence looks overwhelmingly positive.\n",
    "    - \"have\" \"not\" \"had\" \"awesome\" \"crazy\" are words that would stand out, though the algorithm for unigrams doesn't connect the relation between \"not\" and another word.\n",
    "\n",
    "\n",
    "- Bigrams, you begin to understand negations.\n",
    "    - \"have not\" \"not had\" \"an awesome\" \"awesome day\" \"been crazy\"  are bigrams that would stand out. As you can see, the bigram model shows some underlying negations, but still this sentence is scored positively.\n",
    "\n",
    "\n",
    "- Trigrams, you start peeking at the underlying sentence structure\n",
    "    - \"I have not\" \"have not had\" \"not had an\" \"an awesome day\" \"it's been crazy\" are some trigrams that would stand out. Now, we've got many more tokens that include the negation and better capture the essence of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe a commercial application of sentiment analysis:\n",
    "---\n",
    "- Utilizing sentiment analysis to analyze IMDB movie reviews and try to capture the audience response in a broader setting in order to \"mine\" their criticism.\n",
    "\n",
    "- Utilizing sentiment analysis to quantify and identify textual documents that attempt to sway the reader through appeals to emotion. For example, in a news setting, human fallacy leads to misunderstanding; News outlets could score the articles of their journalists to verify that appeals to emotion aren't being utilized rather than fact based content.\n",
    "\n",
    "- Utilizing sentiment analysis to allow users with mental health issues to monitor their own text output in order to identify times when they're experiencing unhealthy sections and notify them of mindful ways to calm themselves down, divert their attention, or otherwise. This could also be extended to speech analysis.\n",
    "\n",
    "- Utilizing sentiment analysis in an active/reactive social policy setting to help governing officials understand the emotion coupled with issues that their constituency faces.\n",
    "\n",
    "- Utilizing sentiment analysis to identify changes in behaviour in a social media setting given outside influences; i.e., how does the psychological make-up of a person's online presence change given different social spheres, and how could that idea be utilized to greater identify chat/spam bots in social media settings?\n",
    "\n",
    "These are just some ideas I've had (except for the IMDB thing, that's been done). If you're interested in one of them, let me know! I'd love to do some applied research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the role of cost and gamma in the radial SVM kernel in terms of flexibility and generalization (overfitting data)?\n",
    "----\n",
    "According to the scikit-learn manual:\n",
    "\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "My words: With lots of data, a low gamma is appropriate; Think about it, if you have lots of data, you do not want all of the data individually influencing the kernel heavily because this would lead to widely varying results. With few data, a higher gamma is more appropriate if you'd like to extend the range of classification of the model.\n",
    "\n",
    "The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.\n",
    "\n",
    "My words: If the C is too high, you risk overfitting, and if the C is too low, you risk underfitting.\n",
    "\n",
    "If you're interested in a more in-depth look at SVM hyperparameters, check out the source:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a small example of using the decision tree for classification.\n",
    "----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
